## Data
### Benchmarks
- [QGEval](./qgeval.xlsx), a recent public meta-evaluation benchmark specifically designed for evaluating automatic metrics in question generation. The benchmark consists of 3,000 (passage, question, answer) triples, each annotated with human ratings across seven evaluation dimensions.

- [SimQG](./simqg.xlsx), a benchmark designed to evaluate question quality, with human annotations across three evaluation dimensions.

- [SQuAD2.0](./squad-1k.xlsx), a reading comprehension dataset containing both answerable and unanswerable questions, enabling evaluation of the answerability dimension in question generation.


### Pilot Set
- [pilot_set](./sample-300.xlsx), the dataset used for our pilot study consists of samples accompanied by human-annotated error labels.


### Error Seed
- [seed](./samples_for_simulation.xlsx), is the example seeds we use for error simulation.


### Unlabeled Data Pool
- [Unlabeled_Pool](https://huggingface.co/datasets/anonymous-11/unlabeled_pool_QG), is the unlabeled data pool containing 170k+ questions generated by multiple QG models.
